{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615fd34d",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Prefce</font>\n",
    "Here we are going to use a toy testing environment `GridWorld` to demonstrate the usage of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68f300",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Importing Packages</font>\n",
    "Firstly, let's import all the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce17785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline import lab\n",
    "from skyline.lab import gridworld_env\n",
    "from skyline.lab import gridworld_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1797ee9",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Make Lab Environment</font>\n",
    "We can list supported environment as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54106b14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== GridWorld =====\n",
      "This is a environment to show case of Skyline lab. The environment is a grid world where you can move up, down, right and leftif you don't encounter obstacle. When you obtain the reward (-1, 1, 2), the game is over. You can use env.info() to learn more.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lab.list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02fb4b",
   "metadata": {},
   "source": [
    "Then We use function <font color='blue'>make</font> to obtain the desired environment. e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff46a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env = lab.make(lab.Env.GridWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33754c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- environment is a grid world\n",
      "- x means you can't go there\n",
      "- s means start position\n",
      "- number means reward at that state\n",
      "===========\n",
      ".  .  .  1\n",
      ".  x  . -1\n",
      ".  .  .  x\n",
      "s  x  .  2\n",
      "===========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Check what our environment looks like:\n",
    "grid_env.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7b6952",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['U', 'D', 'L', 'R']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show available actions\n",
    "grid_env.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2097e787",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridState(i=3, j=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1564b",
   "metadata": {},
   "source": [
    "Let's take a action and check the state change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39fdf52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridState(i=2, j=0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take action 'Up'\n",
    "grid_env.step('U')\n",
    "\n",
    "# Check current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6748196",
   "metadata": {},
   "source": [
    "After taking action `U`, we expect the axis-i to move up from 2->1 and we can confirm it from the output state. Let's reset the environment by calling method <font color='blue'>reset</font> which will bring the state of environment back to intial state `GridState(i=2, j=0)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718ed1ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridState(i=3, j=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset environment\n",
    "grid_env.reset()\n",
    "\n",
    "# Check current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd1109",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Experiments of RL algorithms</font>\n",
    "Here we are going to test some well-known RL algorithms and demonstrate the usage of this lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4293e",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Monte Carlo Method</font>\n",
    "<b><font size='3ptx'>In this method, we simply simulate many trajectories (<font color='darkbrown'>decision processes</font>), and calculate the average returns.</font></b> ([wiki page](https://en.wikiversity.org/wiki/Reinforcement_Learning#Monte_Carlo_policy_evaluation))\n",
    "\n",
    "We implement this algorithm in `monte_carlo.py`. The code below will demonstrate the usage of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bd628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline.lab.alg import monte_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ff2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_alg = monte_carlo.MonteCarlo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364e5aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- environment is a grid world\n",
      "- x means you can't go there\n",
      "- s means start position\n",
      "- number means reward at that state\n",
      "===========\n",
      ".  .  .  1\n",
      ".  x  . -1\n",
      ".  .  .  x\n",
      "s  x  .  2\n",
      "===========\n",
      "\n"
     ]
    }
   ],
   "source": [
    "grid_env.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a89fdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'U'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_env.random_action(gridworld_env.GridState(1, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047af4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 10000/10000 [00:03<00:00, 2538.64it/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "mc_alg.fit(grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c61c69",
   "metadata": {},
   "source": [
    "Let's check what value function we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4294aaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      " 1.18| 1.31| 1.46| 1.00|\n",
      "---------------------------\n",
      " 1.31| 0.00| 1.62|-1.00|\n",
      "---------------------------\n",
      " 1.46| 1.62| 1.80| 0.00|\n",
      "---------------------------\n",
      " 1.31| 0.00| 2.00| 2.00|\n"
     ]
    }
   ],
   "source": [
    "gridworld_utils.print_values(mc_alg._state_2_value, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7f10b",
   "metadata": {},
   "source": [
    "Then let's print the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976fb2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "  R  |  R  |  D  |  ?  |\n",
      "---------------------------\n",
      "  D  |  x  |  D  |  ?  |\n",
      "---------------------------\n",
      "  R  |  R  |  D  |  x  |\n",
      "---------------------------\n",
      "  U  |  x  |  R  |  ?  |\n"
     ]
    }
   ],
   "source": [
    "gridworld_utils.print_policy(mc_alg._policy, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da574b5b",
   "metadata": {},
   "source": [
    "Finally, let's reset the environment and play the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "82be40e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af03368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin state=GridState(i=3, j=0)\n",
      "ActionResult(action='U', state=GridState(i=2, j=0), reward=0, is_done=False, is_truncated=False, info=None)\n",
      "ActionResult(action='R', state=GridState(i=2, j=1), reward=0, is_done=False, is_truncated=False, info=None)\n",
      "ActionResult(action='R', state=GridState(i=2, j=2), reward=0, is_done=False, is_truncated=False, info=None)\n",
      "ActionResult(action='D', state=GridState(i=3, j=2), reward=0, is_done=False, is_truncated=False, info=None)\n",
      "ActionResult(action='R', state=GridState(i=3, j=3), reward=2, is_done=True, is_truncated=False, info=None)\n",
      "Final state=GridState(i=3, j=3)\n"
     ]
    }
   ],
   "source": [
    "# Play game util done\n",
    "print(f'Begin state={grid_env.current_state}')\n",
    "while not grid_env.is_done:\n",
    "    result = mc_alg.play(grid_env)\n",
    "    print(result)\n",
    "print(f'Final state={grid_env.current_state}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "820956ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{GridState(i=0, j=0): 1.1764648192771077,\n",
       " GridState(i=0, j=1): 1.3084620719178073,\n",
       " GridState(i=0, j=2): 1.4580000000000002,\n",
       " GridState(i=1, j=0): 1.3086440381679392,\n",
       " GridState(i=1, j=2): 1.6197544101433314,\n",
       " GridState(i=2, j=0): 1.4580000000000002,\n",
       " GridState(i=2, j=1): 1.6196147443519608,\n",
       " GridState(i=2, j=2): 1.7995799299883288,\n",
       " GridState(i=3, j=0): 1.3068093930421927,\n",
       " GridState(i=3, j=2): 2.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_alg._state_2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ec77577f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{GridState(i=0, j=0): {'U': 0,\n",
       "  'D': 1.1605357643002028,\n",
       "  'L': 0,\n",
       "  'R': 1.1764648192771077},\n",
       " GridState(i=0, j=1): {'U': 0,\n",
       "  'D': 0,\n",
       "  'L': 1.0560951529411764,\n",
       "  'R': 1.3084620719178073},\n",
       " GridState(i=0, j=2): {'U': 0,\n",
       "  'D': 1.4580000000000002,\n",
       "  'L': 1.1764287976539605,\n",
       "  'R': 1.0},\n",
       " GridState(i=1, j=0): {'U': 1.04715191696751,\n",
       "  'D': 1.3086440381679392,\n",
       "  'L': 0,\n",
       "  'R': 0},\n",
       " GridState(i=1, j=2): {'U': 1.3066297297297307,\n",
       "  'D': 1.6197544101433314,\n",
       "  'L': 0,\n",
       "  'R': -1.0},\n",
       " GridState(i=2, j=0): {'U': 1.161856872000001,\n",
       "  'D': 1.1668365269461092,\n",
       "  'L': 0,\n",
       "  'R': 1.4580000000000002},\n",
       " GridState(i=2, j=1): {'U': 0,\n",
       "  'D': 0,\n",
       "  'L': 1.3050743237704914,\n",
       "  'R': 1.6196147443519608},\n",
       " GridState(i=2, j=2): {'U': 1.4530344827586203,\n",
       "  'D': 1.7995799299883288,\n",
       "  'L': 1.4499778441558437,\n",
       "  'R': 0.0},\n",
       " GridState(i=3, j=0): {'U': 1.3068093930421927, 'D': 0, 'L': 0, 'R': 0},\n",
       " GridState(i=3, j=2): {'U': 1.6100205338809028, 'D': 0, 'L': 0, 'R': 2.0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mc_alg._q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cf39c",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Supplement</font>\n",
    "* [Udemy - Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
