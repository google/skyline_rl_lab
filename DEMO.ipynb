{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "615fd34d",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Prefce</font>\n",
    "Here we are going to use a toy testing environment `GridWorld` to demonstrate the usage of this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd68f300",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Importing Packages</font>\n",
    "Firstly, let's import all the necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce17785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline import lab\n",
    "from skyline.lab import gridworld_env\n",
    "from skyline.lab import gridworld_utils\n",
    "from skyline.lab.alg import monte_carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1797ee9",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Make Lab Environment</font>\n",
    "We can list supported environment as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54106b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.list_env()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd02fb4b",
   "metadata": {},
   "source": [
    "Then We use function <font color='blue'>make</font> to obtain the desired environment. e.g.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff46a036",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env = lab.make(lab.Env.GridWorld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33754c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what our environment looks like:\n",
    "grid_env.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d7b6952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show available actions\n",
    "grid_env.available_actions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2097e787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df1564b",
   "metadata": {},
   "source": [
    "Let's take a action and check the state change:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b39fdf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take action 'Up'\n",
    "grid_env.step('U')\n",
    "\n",
    "# Check current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6748196",
   "metadata": {},
   "source": [
    "After taking action `U`, we expect the axis-i to move up from 2->1 and we can confirm it from the output state. Let's reset the environment by calling method <font color='blue'>reset</font> which will bring the state of environment back to intial state `GridState(i=2, j=0)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "718ed1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset environment\n",
    "grid_env.reset()\n",
    "\n",
    "# Check current state\n",
    "grid_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cd1109",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Experiments of RL algorithms</font>\n",
    "Here we are going to test some well-known RL algorithms and demonstrate the usage of this lab:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed4293e",
   "metadata": {},
   "source": [
    "<a id='monte_carlo_method'></a>\n",
    "### <font color='darkgreen'>Monte Carlo Method</font>\n",
    "<b><font size='3ptx'>In this method, we simply simulate many trajectories (<font color='darkbrown'>decision processes</font>), and calculate the average returns.</font></b> ([wiki page](https://en.wikiversity.org/wiki/Reinforcement_Learning#Monte_Carlo_policy_evaluation))\n",
    "\n",
    "We implement this algorithm in `monte_carlo.py`. The code below will demonstrate the usage of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57bd628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline.lab.alg import monte_carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e5ff2c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_alg = monte_carlo.MonteCarlo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "364e5aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a1a89fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_env.random_action(gridworld_env.GridState(1, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8e4155",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047af4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training\n",
    "mc_alg.fit(grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c61c69",
   "metadata": {},
   "source": [
    "Let's check what value function we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4294aaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_utils.print_values(mc_alg._state_2_value, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7f10b",
   "metadata": {},
   "source": [
    "Then let's print the learned policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "976fb2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "gridworld_utils.print_policy(mc_alg._policy, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043e2c13",
   "metadata": {},
   "source": [
    "#### Prior run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da574b5b",
   "metadata": {},
   "source": [
    "Finally, let's reset the environment and play the game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af03368d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play game util done\n",
    "grid_env.reset()\n",
    "\n",
    "print(f'Begin state={grid_env.current_state}')\n",
    "step_count = 0\n",
    "while not grid_env.is_done:\n",
    "    result = mc_alg.play(grid_env)\n",
    "    step_count += 1\n",
    "    print(result)\n",
    "    \n",
    "print(f'Final reward={result.reward} with {step_count} step(s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "820956ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned value function\n",
    "# mc_alg._state_2_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec77577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show learned Q table\n",
    "# mc_alg._q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563e2c89",
   "metadata": {},
   "source": [
    "<a id='random_method'></a>\n",
    "### <font color='darkgreen'>Random Method</font>\n",
    "This method takes random action in the given environment. It is often used as a based line to evaluate other RL methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0dd62d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline.lab.alg import random_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bcdd775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_alg = random_rl.RandomRL()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db98e07",
   "metadata": {},
   "source": [
    "#### Train\n",
    "Random won't require any training and therefore below call should end in no time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2c2097d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training\n",
    "random_alg.fit(grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57673b4a",
   "metadata": {},
   "source": [
    "#### Prior run\n",
    "Since this is a random process, each time you play the game will have difference result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f154d41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play game util done\n",
    "grid_env.reset()\n",
    "\n",
    "print(f'Begin state={grid_env.current_state}')\n",
    "step_count = 0\n",
    "while not grid_env.is_done:\n",
    "    result = random_alg.play(grid_env)\n",
    "    step_count += 1\n",
    "    print(result)\n",
    "print(f'Final reward={result.reward} with {step_count} step(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de31d4e7",
   "metadata": {},
   "source": [
    "From the result above, it is obviously that <a href='#monte_carlo_method'><b>Monte Carlo Method</b></a> can perform much better than <a href='#random_method'><b>Random Method</b></a>!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee12ee4",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Scoreboard</font>\n",
    "Before we know how score board work, we need to understand <b><font color='blue'>RLExaminer</font></b> first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf9e02d",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>RLExaminer</font>\n",
    "Every environment can have more than one examiner to calculate the score of RL method. Each examiner may have its own aspect to evaluate the RL method (time, reward etc.). Let's check one used to calculate the average reward of grid environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0edfceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner = gridworld_env.GridWorldExaminer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c32bee",
   "metadata": {},
   "source": [
    "Then, what's score of `Monte Carlo Method`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ec60250",
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner.score(mc_alg, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3c476a",
   "metadata": {},
   "source": [
    "`Monte Carlo Method` got score 0.4. Let's check another RL method `Random Method`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5a3c08e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examiner.score(random_alg, grid_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c41b83",
   "metadata": {},
   "source": [
    "`Random Method` got score 0.5 which is less than `Monte Carlo Method`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1526f9d9",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Score Board</font>\n",
    "<b><font color='blue'>Scoreboard</font></b> literally calculate the scores of given RL methods according to the specific examiner and the rank those RL methods accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "56eb658b",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_board = lab.Scoreboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "46f80bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores  = score_board.rank(\n",
    "    examiner=examiner, env=grid_env, rl_methods=[random_alg, mc_alg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5b4af04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c461de5",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>A Real World RL problem (BCST test case selection)</font>\n",
    "Here we are going to use a real-world RL example to explain how this lab works. From environment BCST, we want to select test cases strategically so to form a test case execution sequence which will obtain the maximum accumulated reward.\n",
    "\n",
    "From BCST environment:\n",
    "* Each action is a selected test case.\n",
    "* State is the sequence of last executed test case sequence.\n",
    "* A reward equal to 1 means the execution resulted in a crash/ramdump or 0 means nothing was caught."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdcccf2",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Explore the environment</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1a48c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from skyline.lab import bcst_tc_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8c577e",
   "metadata": {},
   "source": [
    "#### Initialize environment and show info of it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c579d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env = bcst_tc_env.BCSTEnvironment()\n",
    "bcst_examiner = bcst_tc_env.BCSTRewardCountExaminer()\n",
    "\n",
    "bcst_env.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8abd7b58",
   "metadata": {},
   "source": [
    "#### Supported actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "af7f7e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.available_actions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21eb0458",
   "metadata": {},
   "source": [
    "#### Available states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d93e2169",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.available_states()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fc487e",
   "metadata": {},
   "source": [
    "#### Take a few actions to execute for experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "953d8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.step('rl_test_case1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a6ea5f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.step('rl_test_case2')\n",
    "bcst_env.step('rl_test_case3')\n",
    "bcst_env.current_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d656f170",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.step('rl_test_case4')\n",
    "bcst_env.step('rl_test_case5')\n",
    "bcst_env.current_state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2316f676",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Random RL</font>\n",
    "Let's check our baseline `Random Method`:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd917d63",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "93c4835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training\n",
    "random_alg.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d81095",
   "metadata": {},
   "source": [
    "#### Prior run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e7d02705",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_env.reset()\n",
    "random_alg.play(bcst_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05542b2e",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "23b66ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, _ = bcst_examiner.score(random_alg, bcst_env, play_round=10)\n",
    "print(f'Score={score:.02f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381b172d",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Monte Carlo Method</font>\n",
    "Next let's check `Monte Carlo Method` with three cases:\n",
    "* `ml_alg_r1000`: Play 1000 times in BCST environment.\n",
    "* `ml_alg_r3000`: Play 3000 times in BCST environment.\n",
    "* `ml_alg_r5000`: Play 5000 times in BCST environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f1c42aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_alg_r1000 = monte_carlo.MonteCarlo(name='mc_r1000', round_num=1000)\n",
    "mc_alg_r3000 = monte_carlo.MonteCarlo(name='mc_r3000', round_num=3000)\n",
    "mc_alg_r5000 = monte_carlo.MonteCarlo(name='mc_r5000', round_num=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49482d40",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94f7ee04",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 1000 times in BCST environment.\n",
    "mc_alg_r1000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "361e6cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 3000 times in BCST environment.\n",
    "mc_alg_r3000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5980288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 5000 times in BCST environment.\n",
    "mc_alg_r5000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be4f2ac",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a8519b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r1000_data = bcst_examiner.score(mc_alg_r1000, bcst_env, play_round=15)\n",
    "print(f'Monte Carlo Method ({mc_alg_r1000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1bc88ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r3000_data = bcst_examiner.score(mc_alg_r3000, bcst_env, play_round=15)\n",
    "print(f'Monte Carlo Method ({mc_alg_r3000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "060b5818",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r5000_data = bcst_examiner.score(mc_alg_r5000, bcst_env, play_round=15)\n",
    "print(f'Monte Carlo Method ({mc_alg_r5000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "04b66081",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('Monte Carlo Method (play_round=15; BCST will execute 500 steps in each round)')\n",
    "ax.boxplot([r1000_data, r3000_data, r5000_data], labels=['r1000', 'r3000', 'r5000'])\n",
    "plt.ylabel('Accumulated reward')\n",
    "plt.xlabel('RL method name')\n",
    "plt.minorticks_on()\n",
    "plt.tick_params(axis='x', which='minor', bottom=False)\n",
    "plt.tick_params(axis='x', which='major', labelsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066996b",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>BCST Test case selector</font>\n",
    "From BCST in G3, we implement our own RL method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d648b79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skyline.lab.alg import bcst_epsilon_tcs_rl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d771e168",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_rl_method_r1000 = bcst_epsilon_tcs_rl.EGreedyStrategy(name='tcs_r1000', round_num=1000)\n",
    "bcst_rl_method_r2000 = bcst_epsilon_tcs_rl.EGreedyStrategy(name='tcs_r1000', round_num=2000)\n",
    "bcst_rl_method_r3000 = bcst_epsilon_tcs_rl.EGreedyStrategy(name='tcs_r1000', round_num=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea85f7f",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d503f5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 1000 times in BCST environment.\n",
    "bcst_rl_method_r1000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924f9530",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 2000 times in BCST environment.\n",
    "bcst_rl_method_r2000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ec3ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Training by playing 3000 times in BCST environment.\n",
    "bcst_rl_method_r3000.fit(bcst_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ca9a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_one_knowledge = bcst_rl_method_r3000.qtable['3'].items()\n",
    "sorted(level_one_knowledge, key=lambda t: -t[1].accumulated_reward)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd9ec0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bcst_rl_method_r3000._run_policy(bcst_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a856ad3",
   "metadata": {},
   "source": [
    "#### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6b35cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r1000_data = bcst_examiner.score(bcst_rl_method_r1000, bcst_env, play_round=30)\n",
    "print(f'BCST TC selector ({bcst_rl_method_r1000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91672994",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r2000_data = bcst_examiner.score(bcst_rl_method_r2000, bcst_env, play_round=30)\n",
    "print(f'BCST TC selector ({bcst_rl_method_r2000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93fdc17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "score, r3000_data = bcst_examiner.score(bcst_rl_method_r3000, bcst_env, play_round=15)\n",
    "print(f'BCST TC selector ({bcst_rl_method_r3000.name}) with average score={score:.02f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6dea78",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_title('BCST TC selector (play_round=30; BCST will execute 500 steps in each round)')\n",
    "ax.boxplot([r1000_data, r2000_data, r3000_data], labels=['r1000', 'r3000', 'r5000'])\n",
    "plt.ylabel('Accumulated reward')\n",
    "plt.xlabel('RL method name')\n",
    "plt.minorticks_on()\n",
    "plt.tick_params(axis='x', which='minor', bottom=False)\n",
    "plt.tick_params(axis='x', which='major', labelsize='small')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfa9c53",
   "metadata": {},
   "source": [
    "### <font color='darkgreen'>Score Board</font>\n",
    "Finally, let's check the ranking among supported RL methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d739fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_scores  = score_board.rank(\n",
    "    examiner=bcst_examiner, env=bcst_env,\n",
    "    rl_methods=[random_alg, mc_alg_r1000, mc_alg_r3000, mc_alg_r5000,\n",
    "                bcst_rl_method_r1000, bcst_rl_method_r2000, bcst_rl_method_r3000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3cf39c",
   "metadata": {},
   "source": [
    "## <font color='darkblue'>Supplement</font>\n",
    "* [Udemy - Artificial Intelligence: Reinforcement Learning in Python](https://www.udemy.com/course/artificial-intelligence-reinforcement-learning-in-python/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
